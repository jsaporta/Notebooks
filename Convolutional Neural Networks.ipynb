{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "Jason Saporta, Department of Statistics, Iowa State University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz41.png\" style=\"margin-left: auto; margin-right: auto;\"></img>\n",
    "A typical neural network has completely connected layers.\n",
    "\n",
    "Each connection is associated with a different weight parameter.\n",
    "\n",
    "That's a lot of parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Data\n",
    "Suppose our inputs have the form of 2-dimensional grayscale images.\n",
    "\n",
    "Then our input layer looks like:\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz42.png\" style=\"margin-left: auto; margin-right: auto;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local Receptive Fields\n",
    "Instead of using a completely connected \"dense\" layer, neurons in the first hidden layer will each only be connected to a small patch of the input image. The size of this patch is dictated by hyperparameter values provided to the algorithm.\n",
    "\n",
    "This patch is called the **local receptive field** for that neuron.\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz43.png\" style=\"margin-left: auto; margin-right: auto;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local Receptive Fields\n",
    "We will have one neuron in the hidden layer for each possible local receptive field in the input image (for now).\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz44.png\" style=\"margin-left: auto; margin-right: auto;\"></img>\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz45.png\" style=\"margin-left: auto; margin-right: auto;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shared Weights\n",
    "The weights connecting each of these local receptive fields to their respective hidden neurons are shared across the hidden layer.\n",
    "\n",
    "This means that our hidden layer is really the result of a **convolution** between the input image and a kernel matrix consisting of learnable values. Convolutions are frequently used in image processing to detect the presence of certain features in an image.\n",
    "\n",
    "Hidden layer neurons will activate when the feature specified by the kernel is present in their respective local receptive fields. The exact details of when they will activate will depend on the kernel, the bias value, and the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagram of a Convolution\n",
    "<img src=\"cnnpics/conv.png\" style=\"margin-left: auto; margin-right: auto;\"  height=\"50%\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Feature Maps\n",
    "So far we have discussed the case where there is one learned kernel that is convolved with the input image to produce the hidden layer.\n",
    "\n",
    "In practice, we usually have multiple kernels, each of which are convolved with the input image. The hidden layer will then consist of multiple **feature maps**.\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz46.png\" style=\"margin-left: auto; margin-right: auto;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling Layers\n",
    "In addition to convolutional layers, we also have **pooling layers**, which frequently come directly after convolutional layers.\n",
    "\n",
    "The distinction between the two is fuzzy, but the idea here is that the *exact location* of a feature in an image is frequently unimportant. This gives us a coarser, *smaller* representation of each feature map computed in the previous layer.\n",
    "\n",
    "Here, the maximum of the input units in the image patch is computed:\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz47.png\" style=\"margin-left: auto; margin-right: auto;\"></img>\n",
    "Pooling is also typically done using averaging the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling Layers\n",
    "Again, this can be replicated separately for each feature map under consideration.\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz48.png\" style=\"margin-left: auto; margin-right: auto;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A (Very Simple) Complete Convolutional Neural Network\n",
    "Armed with the output of the pooling layer, we can imagine adding on a softmax layer (for example) to perform classification.\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz49.png\" style=\"margin-left: auto; margin-right: auto;\"></img>\n",
    "In a real-life network, we will typically have several pairs of convolutional and pooling layers stacked on top of each other, and potentially a fully-connected layer as well before our output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminological Considerations\n",
    "<img src=\"cnnpics/term.png\" style=\"margin-left: auto; margin-right: auto;\" height=\"75%\" width=\"75%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application Areas\n",
    "- Image Classification\n",
    "- Object Detection\n",
    "- Object Tracking\n",
    "- Pose Estimation\n",
    "- Text Detection and Recognition\n",
    "- Visual Saliency Detection\n",
    "- Action Recognition\n",
    "- Scene Labeling\n",
    "- Speech Processing\n",
    "- Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extensions to the Basic Architecture\n",
    "There are at least seven areas where improvements have been proposed for CNNs:\n",
    "- Convolutional Layers\n",
    "- Pooling Layers\n",
    "- Activation Functions\n",
    "- Loss Functions\n",
    "- Regularization Methods\n",
    "- Optimization Methods\n",
    "- CNN Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Layers\n",
    "- **Tiled Convolution:** Uses multiple kernels when creating a single feature map, cycling between them as the sliding window moves across the image. The goal here is to learn features that are invariant to more than just small shifts.\n",
    "- **Deconvolution:** Connects a single input activation to multiple outputs, done by performing a standard convolution on upsampled input values. Useful for visualizing the innards of a CNN by mapping feature maps back to pixel space.\n",
    "- **Network in Network:** Replaces the linear filter of a convolutional layer by a small multilayer perceptron capable of approximating a more abstract representation of latent concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense, Tiled Convolutional, and Standard Convolutional Layers\n",
    "<img src=\"cnnpics/tiled.png\" style=\"margin-left: auto; margin-right: auto;\" height=\"50%\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling Layers\n",
    "- **$\\boldsymbol{L_p}$ pooling**: $$y_{ij} = \\left[ \\sum_{(m, n) \\in \\mathcal{R}_{ij}} a_{(m, n)}^p \\right]^{1 / p}$$ This corresponds to average pooling when $p = 1$ and max pooling when $p = \\infty$.\n",
    "- **Mixed Pooling**: During the forward propagation process, randomly decide whether to use average or mixed pooling. Retain the choice for when you back-propagate through the network.\n",
    "- **Stochastic Pooling:** Randomly select one of the nodes in the pooling region based on a multinoulli distribution, where the node probabilities are determined by scaling the activation values so they sum to 1. The selected node's activation value is then used.\n",
    "- **Spatial Pyramid Pooling:** Pool the image based on size-proportional regions rather than a sliding window. This can output a fixed-length layer even when dealing with images of multiple sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation Functions\n",
    "- **ReLU:** $\\max\\{0, a\\}$. This computes faster than sigmoid or tanh activation functions, and induces sparsity in the learned parameters.\n",
    "- **Leaky ReLU:** $\\max\\{0, a\\} + \\lambda \\min\\{0, a\\}$ for some predefined $\\lambda \\in (0, 1)$. Allows for a non-zero gradient when the unit is inactive.\n",
    "- **Randomized Leaky ReLU:** $\\max\\{0, a\\} + \\lambda \\min\\{0, a\\}$, where $\\lambda \\sim \\text{Unif}(0, 1)$ during forward propagation (for each neuron with this activation function) and this value is saved during back-propagation.\n",
    "- **Maxout:** Max of the weighted inputs over multiple channels (such as color for images). Generalizes the ReLU and absolute value activation functions. Designed for use with dropout regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss Functions\n",
    "- **Hinge Loss:** This can be used to create the equivalent of an SVM classifier which uses the features automatically created in the network.\n",
    "- **Softmax Loss:** This creates the equivalent of a multinoulli GLM using the network's features.\n",
    "- **Contrastive Loss:** Used to train Siamese networks, which are two identical CNNs used together for the purpose of identifying matching and non-matching input values.\n",
    "- **Triplet Loss:** Takes in three data points: an *anchor*, a *positive instance*, and a *negative instance*. The goal here is to minimize the distance between the anchor and the positive instance, and maximize that between the anchor and the negative instance.\n",
    "- **K-L Divergence:** Used to train autoencoders. The symmetric form has been used to train Generative Adversarial Networks (GANs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization Methods\n",
    "- **$\\boldsymbol{L_p}$ Norm Regularization**: A very widespread regularization method; when $p = 2$ this is known as *weight decay*.\n",
    "- **Dropout:** Randomly removes neurons from the network on each iteration of SGD. This prevents the network from becoming too dependent on any small set of neurons, and forces it to be accurate even in the absence of certain information.\n",
    "- **DropConnect:** Similar to Dropout, but randomly sets weights to $0$ rather than removing neurons from the network. The distinction is important because of the shared weights in CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout Diagram\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz31.png\" style=\"margin-left: auto; margin-right: auto;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization Methods\n",
    "- **Data Augmentation:** Deep CNNs are dependent on large quantities of training data. When there is a small amount of data, one might want to *augment* it by adding new data points created from the old ones. This may be done by rotating, shifting, mirroring, etc. the original data.\n",
    "- **(Nesterov) (Momentum) SGD:** SGD is the standard method for training CNNs, and it may be augmented by using a quantity representing velocity. Nesterov momentum switches the order in which the velocity and gradient are updated.\n",
    "- **Batch Normalization:** Makes the normalization of the data part of the model architecture, rather than a separate pre-processing step. This has the effect of normalizing data based on their specific mini-batch rather than the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fast Processing of CNNs\n",
    "- **FFT:** Used to carry out the convolution operation.\n",
    "- **Low Precision/Binarized Operations:** Small parameter updates may contain a lot of redundant information. To reduce the redundancies, it can be useful to restrict some or all of the arithmetic within the neural network to binary operations.\n",
    "- **Weight Compression:** Used to reduce the number of parameters in the convolutional and fully-connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acknowledgements\n",
    "- Nielsen, Neural Networks and Deep Learning, Determination Press, 2015\n",
    "- Goodfellow et al., Deep Learning, MIT Press 2016\n",
    "- Gu et al., Recent Advances in Convolutional Neural Networks, Arxiv 2017\n",
    "- Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Arxiv 2015"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
