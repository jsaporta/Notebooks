{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every description of the EM algorithm I've ever seen sucks. This is a shame, because it's pretty darn simple for anyone familiar with maximum likelihood (ML) or maximum a posteriori (MAP) estimation. I'll focus on the maximum likelihood approach; extending this to MAP estimation requires a very small change I'll cover at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ye Olde Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a statistician. You have some data:\n",
    "$$ \\boldsymbol{y} = [y_1, \\ldots, y_n]^T $$\n",
    "You conjure up some random variables that you think represent the process that generated your data and slap a joint distribution on them, excluding some unknown parameters. You have just created a statistical model:\n",
    "$$ \\boldsymbol{y} \\sim f(\\boldsymbol{y}; \\boldsymbol{\\theta}) $$\n",
    "Time to fit this:\n",
    "$$ \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{y}) = \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\ell(\\boldsymbol{\\theta}; \\boldsymbol{y}) $$\n",
    "Problem over. You win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Missing Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all fine and good, but maybe your maximization problem sucks. Maybe figuring out that likelihood function is difficult. Maybe you really wish that you had some additional data:\n",
    "$$ \\boldsymbol{z} = [z_1, \\ldots, z_m]^T $$\n",
    "If only you had $\\boldsymbol{z}$ in addition to $\\boldsymbol{y}$, you say to yourself, your life would be so much eaiser. Perhaps that likelihood function is much more natural with $\\boldsymbol{z}$ included, and without it you need to take this integral:\n",
    "$$ f(\\boldsymbol{y}; \\boldsymbol{\\theta}) = \\int f(\\boldsymbol{y}, \\boldsymbol{z} ; \\boldsymbol{\\theta}) d \\boldsymbol{z} $$\n",
    "Ain't nobody got time for that. Fortunately, there is a better way..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Really Dumb Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you never heard of this fancy \"EM algorithm\" or other methods for dealing with missing data. We want to maximize $\\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{y}, \\boldsymbol{z})$ with respect to $\\boldsymbol{\\theta}$, but we don't know $\\boldsymbol{z}$. Let's think about the most boneheaded way of tackling this problem:\n",
    "1. Start with some initial value $\\boldsymbol{\\theta}_0$.\n",
    "2. Replace $\\boldsymbol{z}$ with its conditional expectation:\n",
    "$$ \\mathcal{E} (\\boldsymbol{z} | \\boldsymbol{y}) $$\n",
    "where the expectation is taken under the current parameter values $\\boldsymbol{\\theta}_j$.\n",
    "3. Now take:\n",
    "$$ \\boldsymbol{\\theta}_{j + 1} = \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\ell(\\boldsymbol{\\theta}; \\boldsymbol{y}, \\boldsymbol{z}) $$\n",
    "Go back to step 2 and repeat.\n",
    "\n",
    "Summing up, this approach is:<br />\n",
    "conditional expectation $\\rightarrow$ maximum likelihood $\\rightarrow$ conditional expectation $\\rightarrow$ maximum likelihood $\\rightarrow$ conditional expectation $\\rightarrow \\ldots$<br />\n",
    "Iterate until convergence. You may well have been able to think up this algorithm yourself!\n",
    "\n",
    "The EM algorithm is only **one small change away** from this, and it reduces to exactly this in certain situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may look gross, but we can combine steps 2 and 3 above into one equation:\n",
    "$$ \\boldsymbol{\\theta}_{j + 1} = \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\ell(\\boldsymbol{\\theta}; \\boldsymbol{y}, \\mathcal{E}_{\\boldsymbol{\\theta}_j} (\\boldsymbol{z} | \\boldsymbol{y})) $$\n",
    "Here, I just plugged in $\\mathcal{E}_{\\boldsymbol{\\theta}_j} (\\boldsymbol{z} | \\boldsymbol{y}))$ for $\\boldsymbol{z}$, because that's what we replaced $\\boldsymbol{z}$ with in step 2.\n",
    "\n",
    "Here's the small change we need to make to get the EM algorithm:\n",
    "$$ \\boldsymbol{\\theta}_{j + 1} = \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\mathcal{E}_{\\boldsymbol{\\theta}_j} ( \\ell(\\boldsymbol{\\theta}; \\boldsymbol{y}, \\boldsymbol{z}) | \\boldsymbol{y}) $$\n",
    "What did I do? **I switched the order of evaluating the expectation and the log-likelihood.**\n",
    "\n",
    "What does it mean to take the expected value of a function? What we'll get out of this is a new function. To evaluate it for a given $\\boldsymbol{\\theta}$ value, insert your input into the log-likelihood, then \"expect out\" the $\\boldsymbol{z}$ to get your result. That is, this behaves exactly as you would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Estimation via EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to when we decided to fit our model for $\\boldsymbol{y}$. Another method to do that is to dream up a prior $\\pi(\\boldsymbol{\\theta})$. MAP estimation is then performed by:\n",
    "$$ \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} f(\\boldsymbol{\\theta} | \\boldsymbol{y}) = \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{y}) \\pi(\\boldsymbol{\\theta}) = \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\ell(\\boldsymbol{\\theta}; \\boldsymbol{y}) + \\log[\\pi(\\boldsymbol{\\theta})] $$\n",
    "which is just maximum likelihood estimation with a log-prior thrown in. Go through the development of the EM algorithm and you'll see that nothing really changes when that log-prior is there. Therefore, we have an EM version of MAP estimation as well:\n",
    "$$ \\boldsymbol{\\theta}_{j + 1} = \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\mathcal{E}_{\\boldsymbol{\\theta}_j} ( \\ell(\\boldsymbol{\\theta}; \\boldsymbol{y}, \\boldsymbol{z}) | \\boldsymbol{y}) + \\log[\\pi(\\boldsymbol{\\theta})] $$\n",
    "Remember that the $\\log[\\pi(\\boldsymbol{\\theta})]$ is not included in the expectation. If it were, that term would have no effect, and you'd be back to the same types of updates you would use in ML estimation above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
