{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks in Forensics\n",
    "#### Jason Saporta\n",
    "#### 11/7/17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Machine Learning?\n",
    "- Branch of Artificial Intelligence.\n",
    "- Has existed since the late 50's.\n",
    "- Data-driven approach dominant for about the last 20 years (since SVMs).\n",
    "- **Optimization** vs. **Probabilistic Model**-based ML.\n",
    "- **Supervised**, **Unsupervised**, and **Reinforcement** learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Deep Learning?\n",
    "- Branch of Machine Learning.\n",
    "- Usually involves neural networks, but not always.\n",
    "- Key benefit: **Automatic feature extraction**.\n",
    "- Drawbacks: Data-hungry, computationally intensive.\n",
    "- **Feature Hierarchy**: higher level features constructed from lower-level ones.\n",
    "\n",
    "![](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/11/hierarchical_features.png)\n",
    "Image taken from [NVIDIA blog](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are Neural Networks?\n",
    "- A machine learning method for **function approximation**.\n",
    "- Network represents a function from a (vector) input to a (vector) output.\n",
    "- For each node, output $a \\left( \\sum_i w_i x_i \\right)$, where the $x_i$ values come from the previous layers and the $w_i$ are learned weights (parameters). $a(\\cdot)$ is called an **activation function**.\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz1.png)\n",
    "Image from Michael Nielsen's [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of Neural Networks\n",
    "- 3 main classes: **Multilayer Perceptron**, **Convolutional**, and **Recurrent**.\n",
    "- Other varieties: Siamese, GANs, Capsule networks, etc.\n",
    "- Different architectures correspond to different situations.\n",
    "\n",
    "In a nutshell:\n",
    "- Convolutional NNs for computer vision.\n",
    "- Recurrent NNs for time series data.\n",
    "- Siamese NNs for matching problems.\n",
    "- GANs or VAEs for generating new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Neural Networks I\n",
    "- Treat outputs as independently normally distributed and find MLEs.\n",
    "- Easily extended to classification using a logistic activation in final layer.\n",
    "- Almost exclusively use first-order iterative optimization methods.\n",
    "- When data don't fit in memory, use stochastic optimization techniques.\n",
    "- Bayesian approaches are also possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Neural Networks II\n",
    "- **Backpropagation** makes computing derivatives computationally efficient.\n",
    "- Implemented in NN packages, so you don't need to do any manual calculus.\n",
    "- Training alternates between forward and backward passes.\n",
    "- Many computations $\\rightarrow$ use GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Neural Networks III\n",
    "- NN packages take care of autodiff, GPU management, etc. They use CUDA C++ behind the scenes.\n",
    "- Broadly applicable: Use when you want fast matrix (tensor) operations and don't want to write raw C++.\n",
    "\n",
    "<center><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" height=65% width=65%></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.099712034922046583, 0.97450000000000003]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784) / 255\n",
    "x_test = x_test.reshape(-1, 784) / 255\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(100, input_shape=(784, ), activation='relu'))\n",
    "model.add(Dense(10, input_shape=(100, ), activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "print(model.evaluate(x_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Use (Deep) Neural Networks?\n",
    "In almost every field where machine learning has been applied, you can find claims of better results with deep neural networks. In particular:\n",
    "- Image Recognition\n",
    "- Game Playing (Atari, Go, etc.)\n",
    "- Speech Recognition\n",
    "- Automatic Translation\n",
    "- Forensics Applications!\n",
    "\n",
    "In general: Consider deep learning when you have **lots of data** and expect that you will get the best performance from **complex features** that are unclear how to create by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Handwriting\n",
    "- I couldn't find clear NN-based results for handwriting matching.\n",
    "- See [Graves 2013](https://arxiv.org/abs/1308.0850) for examples of handwriting generation taking style into account.\n",
    "- Approaches to handwriting problems involve recurrent networks with LSTM (long short-term memory) cells.\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "Image from [Chris Olah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shoeprints\n",
    "- [Cross-Domain Forensic Shoeprint Matching](https://vision.ics.uci.edu/papers/KongSRF_BMVC_2017/KongSRF_BMVC_2017.pdf) from CSAFE UCI.\n",
    "- Goal: Determine which type of shoe left an impression found at a crime scene.\n",
    "- Method: Put low- and mid- features from a pre-trained CNN into a Siamese network with a custom matching function, train end-to-end to fine-tune the weights.\n",
    "- Outcome: State-of-the-art performance.\n",
    "\n",
    "![](https://vision.ics.uci.edu/papers/KongSRF_BMVC_2017/icon_drop.jpg)\n",
    "\n",
    "Can this network be modified to get good results on other shoe-matching problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bullets\n",
    "- A little research from the early 2000's on NNs for bullet matching using a simple multilayer perceptron. Results are unclear; few experiments seem to have been performed.\n",
    "\n",
    "Idea:\n",
    "- We can use recurrent neural networks to take in bullet signatures and output a learned feature representation. Two of these representations could be put into a matching function with some other trainable parameters. Train the whole thing end-to-end with logistic loss and we should have a good method for matching bullets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Steganography/Steganalysis\n",
    "- Neural networks have been used for both hiding messages in images and trying to find them.\n",
    "- Use GANs to create convincing images containing hidden messages.\n",
    "- Approaches to steganalysis involve using convolutional neural networks with custom activation functions to tease out small changes to pixels.\n",
    "\n",
    "Ideas:\n",
    "- Do these approaches work with realistic images?\n",
    "- Is it even possible to detect messages without some knowledge of the encoding algorithm?\n",
    "- Credible sets for the predictions generated from these algorithms."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
