{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feed_dict` is bad. As TensorFlow's [reading data guide](https://www.tensorflow.org/api_guides/python/reading_data) says, feeding is inefficient and should only be used for small experiments and debugging. Before TensorFlow 1.2, the main alternative was a [complex queue-based system](https://www.tensorflow.org/programmers_guide/threading_and_queues) which felt a bit too difficult for average humans (or at least for your humble author). The situation has greatly improved with the emergence of the [`Dataset` API](https://www.tensorflow.org/programmers_guide/datasets), but as good as the official guide is, it can still be a bit unclear how to use this system to implement complex data pipelines. The goal of this notebook is to supplement the official guide and help you (and me!) think about these situations. I will restrict myself to the most basic kind of iterator because it plays nicely with TensorFlow's higher-level APIs. I recommend you never write raw TensorFlow code unless you *absolutely have to*.\n",
    "\n",
    "This guide was written in TensorFlow 1.3. `Dataset`s are still in `tf.contrib`-land, which means that the API is not guaranteed to be stable for future releases. It's also still a **work in progress**. Be warned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.data as tfdat\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.set_random_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will make a one shot iterator and print out data values until the provided dataset is empty. It will be used a lot in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_through(dataset):\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(iterator.get_next()))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first consider a single data source. That is, we will not be considering pairs of data points, which might be used in a Siamese neural network, or triplets that might be used in a algorithm using triplet loss. I will use an extremely small, simple dataset so we can be clear about exactly what happens when we create more `Dataset` objects later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.arange(4)\n",
    "ds = tfdat.Dataset.from_tensor_slices(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unmodified Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will iterate through the dataset unmodified, just to be crystal-clear about what we're discussing here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "run_through(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset -> Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As advertised, the `repeat(n)` method will repeat the dataset sequence n times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "ds_rep = ds.repeat(2)\n",
    "run_through(ds_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset -> Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buffer Size >= n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `shuffle()` with the same buffer size as the dataset to ensure that all of the data are randomly shuffled. You are, however, guaranteed to get the entire dataset. So, for example, there is no random seed where this will produce two 3's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ds_shuff = ds.shuffle(4)\n",
    "run_through(ds_shuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ds_big_shuff = ds.shuffle(10)\n",
    "run_through(ds_big_shuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buffer Size < n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a smaller buffer size means that you won't get a truly random shuffle from the dataset. In this stupid example:\n",
    "1. The buffer is filled with data, which comes in order from the dataset. 0 is added to the buffer.\n",
    "2. The iterator receives a `get_next()` command and randomly selects a value from the buffer. The only thing there is 0, so it produces 0.\n",
    "3. The buffer is now empty. The next element in the original sequence (the 1) is added.\n",
    "4. The iterator produces 1.\n",
    "\n",
    "etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "ds_small_shuff = ds.shuffle(1)\n",
    "run_through(ds_small_shuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a slightly larger buffer size and run through the dataset multiple times. Note how the value 2 will never appear as the first element, and 3 will never appear as one of the first two elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "ds_not_as_small_shuff = ds.shuffle(2)\n",
    "run_through(ds_not_as_small_shuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ds_not_as_small_shuff = ds.shuffle(2)\n",
    "run_through(ds_not_as_small_shuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ds_not_as_small_shuff = ds.shuffle(2)\n",
    "run_through(ds_not_as_small_shuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "ds_not_as_small_shuff = ds.shuffle(2)\n",
    "run_through(ds_not_as_small_shuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while not providing true uniform randomness, having a buffer that's smaller than the size of your dataset is typical in \"big data\" situations where you cannot fit everything in memory. This is therefore an important case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset -> Shuffle -> Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see where things begin to get a bit confusing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ds_shuff_rep = ds.shuffle(4).repeat(2)\n",
    "run_through(ds_shuff_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened here? We got the same shuffle on both of our repeats! Was it just bad luck? No, and as it turns out, we get the same effect even with a smaller buffer size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ds_shuff_rep = ds.shuffle(2).repeat(2)\n",
    "run_through(ds_shuff_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset -> Repeat -> Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same does not happen when we do things in this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "ds_rep_shuff = ds.repeat(2).shuffle(4)\n",
    "run_through(ds_rep_shuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "ds_rep_small_shuff = ds.repeat(2).shuffle(2)\n",
    "run_through(ds_rep_small_shuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the buffer is at most the size of the dataset, it appears we will (at least fairly frequently) get one full copy of the dataset per epoch. Use a larger buffer size to avoid this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ds_rep_small_shuff = ds.repeat(2).shuffle(8)\n",
    "run_through(ds_rep_small_shuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my own work, I am working with Siamese networks, which require a pair of training examples to be input to the network. The network performs operations on both of these inputs in parallel and computes a similarity score based on them. You can see that a Siamese network working on a pair of examples is not the same as a garden-variety network working on, say, a 2-example mini-batch. The latter case does not require any calculations which require both of the examples at the same time, at least not before the gradient update step.\n",
    "\n",
    "The examples here will generalize to still more complex cases requiring three or more examples at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 1)\n",
      "(2, 2)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "ds1 = tfdat.Dataset.from_tensor_slices(dat)\n",
    "ds2 = tfdat.Dataset.from_tensor_slices(dat)\n",
    "ds = tfdat.Dataset.zip((ds1, ds2))\n",
    "run_through(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle -> Repeat -> Zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same order in both epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(2, 0)\n",
      "(3, 1)\n",
      "(0, 3)\n",
      "(1, 2)\n",
      "(2, 0)\n",
      "(3, 1)\n",
      "(0, 3)\n"
     ]
    }
   ],
   "source": [
    "ds = tfdat.Dataset.zip((ds1.shuffle(4).repeat(2),\n",
    "                        ds2.shuffle(4).repeat(2))\n",
    "                      )\n",
    "run_through(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat -> Shuffle -> Zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better. No guarantee that we'll have each example from each source in each epoch, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(0, 0)\n",
      "(3, 1)\n",
      "(1, 2)\n",
      "(2, 1)\n",
      "(0, 3)\n",
      "(3, 0)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "ds = tfdat.Dataset.zip((ds1.repeat(2).shuffle(4),\n",
    "                        ds2.repeat(2).shuffle(4))\n",
    "                      )\n",
    "run_through(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(2, 3)\n",
      "(1, 1)\n",
      "(2, 1)\n",
      "(1, 3)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "ds = tfdat.Dataset.zip((ds1.repeat(2).shuffle(4),\n",
    "                        ds2.repeat(2).shuffle(4))\n",
    "                      )\n",
    "run_through(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle -> Concat -> Zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create many datasets and shuffle them, then [reduce](https://docs.python.org/3/library/functools.html#functools.reduce) and zip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(2, 3)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 2)\n",
      "(1, 0)\n",
      "(2, 1)\n",
      "(3, 3)\n",
      "(0, 2)\n",
      "(1, 3)\n",
      "(3, 1)\n",
      "(2, 0)\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def create_pairs_dataset(num_epochs):\n",
    "    ds1_list = [tfdat.Dataset.from_tensor_slices(dat).shuffle(4) for _ in range(num_epochs)]\n",
    "    ds2_list = [tfdat.Dataset.from_tensor_slices(dat).shuffle(4) for _ in range(num_epochs)]\n",
    "    ds1 = reduce(lambda a, b: a.concatenate(b), ds1_list)\n",
    "    ds2 = reduce(lambda a, b: a.concatenate(b), ds2_list)\n",
    "    ds = tfdat.Dataset.zip((ds1, ds2))\n",
    "    return ds\n",
    "    \n",
    "run_through(create_pairs_dataset(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuple_dataset(dat, tuple_size, num_epochs, buffer_size):\n",
    "    ds_list = [None] * tuple_size\n",
    "    for i in range(tuple_size):\n",
    "        set_i_list = [tfdat.Dataset.from_tensor_slices(dat).shuffle(buffer_size) for _ in range(num_epochs)]\n",
    "        set_i = reduce(lambda a, b: a.concatenate(b), set_i_list)\n",
    "        ds_list[i] = set_i\n",
    "    return tfdat.Dataset.zip(tuple(ds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 0)\n",
      "(1, 2, 2)\n",
      "(2, 0, 3)\n",
      "(3, 3, 1)\n",
      "(0, 1, 3)\n",
      "(2, 2, 1)\n",
      "(3, 3, 0)\n",
      "(1, 0, 2)\n",
      "(2, 2, 0)\n",
      "(0, 0, 1)\n",
      "(3, 1, 3)\n",
      "(1, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "run_through(create_tuple_dataset(dat, 3, 3, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
